# Developer Guide

This guide provides a comprehensive overview of the project's structure, architecture, and advanced usage. It is intended for developers who want to understand the project's inner workings. For information on how to contribute, please see the [Contributing Guide](CONTRIBUTING.md).

## 1. Developer Quickstart

This section provides a step-by-step guide to setting up and running the project locally for development.

### Prerequisites

-   Python 3.12+
-   Docker Engine 24+ and Docker Compose v2
-   `make` (optional, but recommended for convenience)

### Local Setup

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/saint2706/Attendance-Management-System-Using-Face-Recognition.git
    cd Attendance-Management-System-Using-Face-Recognition
    ```

2.  **Create and activate a virtual environment:**

    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

3.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up environment variables:**

    Create a `.env` file in the project root and add the following variables. Use the snippet below to generate cryptographically strong keys; keep the encryption keys stable across runs or the encrypted face JPEGs and cached embeddings will become unreadable.

    ```bash
    python - <<'PY'
    from cryptography.fernet import Fernet
    import secrets

    print('DJANGO_SECRET_KEY=', secrets.token_urlsafe(50))
    print('DATA_ENCRYPTION_KEY=', Fernet.generate_key().decode())
    print('FACE_DATA_ENCRYPTION_KEY=', Fernet.generate_key().decode())
    PY
    ```

    Then populate `.env` using the generated values:

    ```
    DJANGO_DEBUG=1
    DJANGO_SECRET_KEY='your-secret-key'
    DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1
    DATA_ENCRYPTION_KEY='your-data-encryption-key'
    FACE_DATA_ENCRYPTION_KEY='your-face-data-encryption-key'
    SENTRY_DSN='' # Optional: for error tracking
    DJANGO_SETTINGS_MODULE='attendance_system_facial_recognition.settings.development'
    ```

    - `DATA_ENCRYPTION_KEY` secures cached embeddings and non-face payloads.
    - `FACE_DATA_ENCRYPTION_KEY` encrypts the face image blobs under `face_recognition_data/` and the synthetic samples generated by `make demo`.

5.  **Run database migrations:**

    ```bash
    python manage.py migrate
    ```

6.  **Create a superuser:**

    ```bash
    python manage.py createsuperuser
    ```

7.  **Run the development server:**

    ```bash
    python manage.py runserver
    ```

    The application will be available at `http://127.0.0.1:8000/`.

### Docker Setup

1.  **Build and run the Docker containers:**

    ```bash
    docker compose up --build
    ```

2.  **Run database migrations:**

    ```bash
    docker compose exec web python manage.py migrate
    ```

3.  **Create a superuser:**

    ```bash
    docker compose exec web python manage.py createsuperuser
    ```

    The application will be available at `http://127.0.0.1:8000/`.

### Running Tests

-   **Locally:**

    ```bash
    pytest
    ```

-   **In Docker:**

    ```bash
    docker compose exec web pytest
    ```

## 2. Project Structure

The project is organized into the following directories:

-   `attendance_system_facial_recognition`: The main Django project directory.
-   `recognition`: The Django app that handles face recognition and attendance tracking.
-   `users`: The Django app that handles user management.
-   `face_recognition_data`: The directory where the face recognition data is stored.
-   `sample_data`: A fully synthetic dataset with three demo identities. It mirrors the `face_recognition_data/training_dataset/`
    layout so you can run the evaluation suite without encrypted production assets. Use `make demo` or
    `python scripts/bootstrap_demo.py` to regenerate the encrypted JPEGs into both locations when they are missing locally.

## 3. Architecture Overview

For a detailed overview of the system architecture, including diagrams and data flow, please see the [Architecture Overview](ARCHITECTURE.md) document.

## 4. Management Commands

The project includes several custom Django management commands for evaluation and analysis.

### Data Preparation

To prepare the data for training and evaluation, use the `prepare_splits` command:

```bash
python manage.py prepare_splits --seed 42
```

### Evaluation

To run a comprehensive evaluation of the model, use the `eval` command. It loads the encrypted training dataset, performs recognition against the cached embeddings, and saves metrics, confusion matrices, and threshold sweeps under `reports/evaluation/`.

```bash
python manage.py eval --split-csv reports/splits.csv --threshold 0.4
```

Key options:

- `--split-csv`: CSV created by `prepare_splits` to restrict evaluation to the held-out test set.
- `--threshold`: Override the main distance threshold (defaults to `RECOGNITION_DISTANCE_THRESHOLD`).
- `--threshold-start/stop/step`: Configure the sweep range for FAR/FRR vs threshold plots.
- `--max-samples`: Quickly smoke-test the pipeline by limiting the number of processed images.
- `--reports-dir`: Customise where artifacts such as `metrics_summary.json`, `confusion_matrix.png`, and `threshold_sweep.csv` are saved.

For reviewers, the quickest path is to reuse the sample dataset:

```bash
python scripts/reproduce_sample_results.py  # equivalent to `make reproduce`
```

The script patches the dataset root in-memory so production deployments remain untouched. Pass `--dataset-root` and `--split-csv`
to point the same workflow at real encrypted datasets once they are available locally.

### Face-matching metric

Both the live service and the evaluation harness compute cosine similarity between embeddings:

- `sim(A, B) = (A · B) / (||A|| ||B||)`
- `d(A, B) = 1 − sim(A, B)`

Attendance is recorded when `d(A, B) ≤ RECOGNITION_DISTANCE_THRESHOLD` (default **0.4**). The `--threshold` flag pins the decision boundary for a single run, while `--threshold-start/stop/step` sweeps a range and writes FAR/FRR trade-offs to `reports/evaluation/threshold_sweep.csv`. Review those plots before changing the global threshold in settings or `.env` files so deployments stay within policy.

### Threshold Selection

To select the optimal recognition threshold, use the `threshold_select` command:

```bash
python manage.py threshold_select --method eer --seed 42
```

### Ablation Experiments

To run ablation experiments, use the `ablation` command:

```bash
python manage.py ablation --seed 42
```

### Export Reports

To export all generated reports and figures, use the `export_reports` command:

```bash
python manage.py export_reports
```

### Liveness Evaluation

Use the `evaluate_liveness` command to sanity-check the motion-based anti-spoofing layer before rolling out new thresholds:

```bash
python manage.py evaluate_liveness --samples-root /path/to/liveness_samples
```

Key options:

- `--samples-root`: Directory that contains `genuine/` and `spoof/` sub-folders, each with short frame sequences.
- `--threshold`: Override `RECOGNITION_LIVENESS_MOTION_THRESHOLD` for the evaluation run only.
- `--min-frames`: Override `RECOGNITION_LIVENESS_MIN_FRAMES` so you can gauge how sensitive the detector is to longer/shorter bursts.

## 5. Makefile Targets

The project includes a comprehensive `Makefile` for common development tasks.

### Development

-   `make run`: Start the Django development server
-   `make migrate`: Run database migrations

### Testing and Evaluation

-   `make test`: Run all Django tests
-   `make evaluate`: Run performance evaluation with metrics and drop artifacts in `reports/evaluation/`
-   `make ablation`: Run ablation experiments
-   `make report`: Generate comprehensive reports (metrics summary, confusion matrix, FAR/FRR sweep)
-   `make reproduce`: Evaluate the bundled synthetic dataset and place deterministic artifacts in `reports/sample_repro/`

## 6. API Reference

For a complete reference of all API endpoints and command-line tools, please see the [API Reference](API_REFERENCE.md).

## 7. Configuration

The system can be configured using environment variables. For a detailed list of all configuration options, please see the [main README file](README.md#deployment-configuration). Operators planning production deployments should also review the [Deployment Guide](DEPLOYMENT.md) for Compose setup, migrations, and hardening recommendations.

New liveness-related toggles:

- `RECOGNITION_LIGHTWEIGHT_LIVENESS_ENABLED` – turn the motion gate on/off without touching DeepFace's anti-spoofing setting.
- `RECOGNITION_LIVENESS_WINDOW` – number of frames to keep in memory for the optical-flow detector.
- `RECOGNITION_LIVENESS_MIN_FRAMES` – minimum frames required before motion can be scored.
- `RECOGNITION_LIVENESS_MOTION_THRESHOLD` – minimum average motion magnitude required to accept the liveness probe.

## 8. Database backends

### Local PostgreSQL with Docker Compose

1. Copy `.env.example` to `.env` and update the secrets as required. The default values match the credentials exported by the Compose profile.
2. Start the containerised database:
   ```bash
   docker compose up -d postgres
   ```
3. Run migrations and tests against the running Postgres instance:
   ```bash
   python manage.py migrate
   pytest
   ```
4. Tear the database down when you are finished:
   ```bash
   docker compose down
   ```

### Continuous Integration

CI pipelines must export `DATABASE_URL` before running `pytest` so Django connects to Postgres instead of the default SQLite fallback. A typical GitHub Actions job includes a Postgres service container and the following step:

```yaml
- name: Run tests
  env:
    DATABASE_URL: postgresql://postgres:postgres@localhost:5432/postgres
  run: pytest
```

Running tests against Postgres ensures migrations stay compatible with the production backend and catches issues that do not appear with SQLite.

## 9. Frontend utilities

### CameraManager helper

Use the `CameraManager` utility (`recognition/static/js/camera.js`) to initialise and tear down shared camera streams in templates. The helper reuses a single `MediaStream` and automatically releases tracks when you call `stop()`.

```html
{% load static %}
<script type="module">
  import { CameraManager } from "{% static 'js/camera.js' %}";

  const cameraManager = new CameraManager();
  const preview = document.querySelector("video");

  async function openPreview() {
    await cameraManager.start(preview);
  }

  function closePreview() {
    cameraManager.stop();
  }
</script>
```

Always pair preview components with lifecycle events (`hidden.bs.modal`, `beforeunload`, etc.) that call `stop()` so the browser can release the webcam promptly.

## 10. UI & Diagnostics

The application includes several admin-only dashboards that help developers debug operational issues without leaving the browser.

### System Health Dashboard

The System Health page (`/admin/health/`) provides a quick overview of:

- **Dataset freshness** – when face images were last added or updated.
- **Model status** – whether the recognition pipeline has loaded embeddings.
- **Recent recognition activity** – a summary of recent recognition attempts with success/failure counts.
- **Celery worker reachability** – confirms that background task workers are responding.

Use this dashboard to verify camera connectivity and model status after deploying changes or restarting services.

### Performance Profiling with Silk

The Silk profiler (`/silk/`) captures detailed request timings, database query counts, and cache hits/misses. In development mode (`DJANGO_DEBUG=1`), the dashboard is accessible without authentication. In production, only staff users can view profiles.

Key debugging workflows:

1. **Slow page loads** – check the "Most time overall" view to identify bottlenecks.
2. **N+1 queries** – the "Most queries" view highlights views that issue excessive database calls.
3. **Cache misses** – review individual requests to see which cache keys are hit or missed.

### Fairness and Evaluation Dashboards

- **Fairness Dashboard** (`/admin/fairness/`) – displays per-group metrics (role, site, source, lighting) from the latest fairness audit.
- **Evaluation Dashboard** (`/admin/evaluation/`) – shows model performance metrics, confusion matrices, and threshold sweep results.

These views help developers understand how recognition accuracy varies across different conditions and user groups.

## 11. Regenerating Documentation Screenshots

The repository includes a helper script to capture a canonical set of screenshots for documentation. This ensures screenshots stay up-to-date as the UI evolves.

### Prerequisites

1. Install Playwright:
   ```bash
   pip install pytest-playwright
   playwright install chromium
   ```

2. Bootstrap the demo environment:
   ```bash
   make demo
   ```

3. Start the development server:
   ```bash
   python manage.py runserver
   ```

### Capturing screenshots

With the server running, execute:

```bash
make docs-screenshots
```

Or run the script directly:

```bash
python scripts/capture_screenshots.py
```

### What the script captures

The script logs in with the demo admin credentials and captures:

| Screenshot | Path | Description |
|------------|------|-------------|
| Home page | `docs/screenshots/home.png` | Landing page with primary actions |
| Login page | `docs/screenshots/login.png` | User authentication screen |
| Admin dashboard | `docs/screenshots/admin-dashboard.png` | Admin home with first-run checklist |
| Employee registration | `docs/screenshots/employee-registration.png` | Form for registering new employees |
| Employee enrollment | `docs/screenshots/employee-enrollment.png` | Photo capture for face recognition |
| Attendance session | `docs/screenshots/attendance-session.png` | Live recognition feed and logs |
| Reports | `docs/screenshots/reports.png` | Attendance reports dashboard |
| System health | `docs/screenshots/system-health.png` | Operational health dashboard |
| Fairness dashboard | `docs/screenshots/fairness-dashboard.png` | Per-group fairness metrics |
| Evaluation dashboard | `docs/screenshots/evaluation-dashboard.png` | Model performance metrics |

### Options

```bash
python scripts/capture_screenshots.py --help
```

- `--base-url` – change the server URL (default: `http://127.0.0.1:8000`)
- `--output-dir` – change the output directory (default: `docs/screenshots/`)
- `--admin-username` / `--admin-password` – use different credentials
- `--headed` – show the browser window during capture (useful for debugging)

### Manual screenshot updates

If you prefer to capture screenshots manually:

1. Run the demo environment and log in with `demo_admin` / `demo_admin_pass`.
2. Navigate to each screen and capture using your browser or OS screenshot tools.
3. Save images to `docs/screenshots/` using the filenames listed above.
4. Ensure images are reasonably sized (1280×800 or similar) for consistency.
