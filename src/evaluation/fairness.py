"""Fairness and robustness metrics for the face-recognition pipeline."""

from __future__ import annotations

import csv
import datetime as dt
import logging
from collections import Counter, defaultdict
from dataclasses import dataclass, replace
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Mapping, Optional, Sequence

from django.conf import settings
from django.db.models import Count

from src.evaluation.face_recognition_eval import (
    UNKNOWN_LABEL,
    EvaluationConfig,
    EvaluationSummary,
    SampleEvaluation,
    compute_basic_metrics,
    run_face_recognition_evaluation,
)

logger = logging.getLogger(__name__)


@dataclass
class SampleContext:
    """Metadata describing a sample's audit group membership."""

    username: str
    role_bucket: str
    site_bucket: str
    source_bucket: str
    lighting_bucket: str


@dataclass
class AnnotatedSample:
    """Pair a :class:`SampleEvaluation` instance with its :class:`SampleContext`."""

    sample: SampleEvaluation
    context: SampleContext


@dataclass
class FairnessAuditConfig:
    """Configuration parameters for the fairness audit."""

    reports_dir: Optional[Path] = None
    dataset_root: Optional[Path] = None
    test_split_csv: Optional[Path] = None
    threshold: Optional[float] = None
    limit_samples: Optional[int] = None

    def __post_init__(self) -> None:
        if getattr(settings, "configured", False):
            base_dir = Path(getattr(settings, "BASE_DIR", Path.cwd()))
        else:
            base_dir = Path.cwd()
        if self.reports_dir is None:
            self.reports_dir = base_dir / "reports" / "fairness"
        else:
            self.reports_dir = Path(self.reports_dir)

        if self.dataset_root is not None:
            self.dataset_root = Path(self.dataset_root)
        if self.test_split_csv is not None:
            self.test_split_csv = Path(self.test_split_csv)
        if self.limit_samples is not None and self.limit_samples <= 0:
            raise ValueError("limit_samples must be positive when provided")

    @property
    def evaluation_reports_dir(self) -> Path:
        """Directory used when persisting intermediate evaluation outputs."""

        assert self.reports_dir is not None
        return self.reports_dir / "evaluation_snapshot"

    def to_evaluation_config(self) -> EvaluationConfig:
        """Translate the audit configuration into :class:`EvaluationConfig`."""

        return EvaluationConfig(
            reports_dir=self.evaluation_reports_dir,
            dataset_root=self.dataset_root,
            test_split_csv=self.test_split_csv,
            threshold=self.threshold,
            limit_samples=self.limit_samples,
        )


@dataclass
class GroupMetrics:
    """Container describing the metrics computed for a single grouping axis."""

    name: str
    rows: List[Dict[str, object]]
    csv_path: Path


@dataclass
class FairnessAuditResult:
    """Outputs generated by :func:`run_fairness_audit`."""

    evaluation: EvaluationSummary
    group_metrics: Dict[str, GroupMetrics]
    summary_path: Path


def estimate_lighting_bucket(image_path: Path) -> str:
    """Return a coarse lighting bucket for the provided image path."""

    try:
        import numpy as np
        from PIL import Image  # Imported lazily to avoid mandatory dependency during tests
    except Exception:  # pragma: no cover - Pillow is part of runtime requirements
        return "unknown"

    try:
        with Image.open(image_path) as img:
            grayscale = img.convert("L")
            data = np.asarray(grayscale, dtype=float)
    except Exception:
        return "unknown"

    if data.size == 0:
        return "unknown"

    mean_brightness = float(data.mean())
    if mean_brightness < 60:
        return "low_light"
    if mean_brightness < 160:
        return "moderate_light"
    return "bright_light"


def _bucketize_role(user) -> str:
    if user is None:
        return "unregistered"
    if user.is_superuser or user.is_staff:
        return "staff_or_admin"
    return "employee"


def _resolve_user_contexts(usernames: Iterable[str]) -> Dict[str, SampleContext]:
    """Return cached metadata describing each username's grouping context."""

    unique_usernames = sorted({name for name in usernames if name and name != UNKNOWN_LABEL})
    from django.contrib.auth import get_user_model

    from users.models import RecognitionAttempt

    user_model = get_user_model()
    user_map: Dict[str, SampleContext] = {}

    if not unique_usernames:
        return user_map

    users = user_model.objects.filter(username__in=unique_usernames)
    for user in users:
        user_map[user.username] = SampleContext(
            username=user.username,
            role_bucket=_bucketize_role(user),
            site_bucket="unspecified",
            source_bucket="unspecified",
            lighting_bucket="unknown",
        )

    # Ensure all usernames have at least a placeholder entry.
    for username in unique_usernames:
        user_map.setdefault(
            username,
            SampleContext(
                username=username,
                role_bucket="unregistered",
                site_bucket="unspecified",
                source_bucket="unspecified",
                lighting_bucket="unknown",
            ),
        )

    attempts = (
        RecognitionAttempt.objects.filter(username__in=unique_usernames)
        .exclude(site="")
        .values("username", "site")
        .annotate(total=Count("id"))
    )
    site_counts: Dict[str, Counter[str]] = defaultdict(Counter)
    for entry in attempts:
        site_counts[entry["username"]][entry["site"]] += entry["total"]

    source_attempts = (
        RecognitionAttempt.objects.filter(username__in=unique_usernames)
        .exclude(source="")
        .values("username", "source")
        .annotate(total=Count("id"))
    )
    source_counts: Dict[str, Counter[str]] = defaultdict(Counter)
    for entry in source_attempts:
        source_counts[entry["username"]][entry["source"]] += entry["total"]

    for username, context in user_map.items():
        if site_counts.get(username):
            context.site_bucket = site_counts[username].most_common(1)[0][0]
        if source_counts.get(username):
            context.source_bucket = source_counts[username].most_common(1)[0][0]

    return user_map


def annotate_samples(samples: Sequence[SampleEvaluation]) -> List[AnnotatedSample]:
    """Attach metadata-derived contexts to each evaluation sample."""

    usernames = [sample.ground_truth for sample in samples]
    context_map = _resolve_user_contexts(usernames)

    annotated: List[AnnotatedSample] = []
    default_contexts: Dict[str, SampleContext] = {}

    for sample in samples:
        username = sample.ground_truth
        if username not in context_map:
            default_contexts.setdefault(
                username,
                SampleContext(
                    username=username,
                    role_bucket="unregistered" if username != UNKNOWN_LABEL else UNKNOWN_LABEL,
                    site_bucket="unspecified",
                    source_bucket="unspecified",
                    lighting_bucket="unknown",
                ),
            )
        base_context = context_map.get(username, default_contexts[username])
        context = replace(base_context)
        context.lighting_bucket = estimate_lighting_bucket(sample.image_path)
        annotated.append(AnnotatedSample(sample=sample, context=context))

    return annotated


def _group_samples(
    annotated_samples: Sequence[AnnotatedSample],
    *,
    value_getter: Callable[[SampleContext], str],
) -> Dict[str, List[SampleEvaluation]]:
    grouped: Dict[str, List[SampleEvaluation]] = defaultdict(list)
    for annotated in annotated_samples:
        group_value = value_getter(annotated.context) or "unspecified"
        grouped[group_value].append(annotated.sample)
    return grouped


def compute_group_metrics(
    annotated_samples: Sequence[AnnotatedSample],
    *,
    threshold: float,
    group_name: str,
    value_getter: Callable[[SampleContext], str],
) -> List[Dict[str, object]]:
    """Compute metrics for each bucket defined by ``value_getter``."""

    grouped = _group_samples(annotated_samples, value_getter=value_getter)
    rows: List[Dict[str, object]] = []
    for group_value, group_samples in sorted(grouped.items()):
        if not group_samples:
            continue
        metrics, _, _, _ = compute_basic_metrics(group_samples, threshold)
        row: Dict[str, object] = {"group": group_value, **metrics}
        rows.append(row)

    logger.info("Computed %s buckets for %s", len(rows), group_name)
    return rows


def _write_group_csv(rows: Sequence[Mapping[str, object]], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if not rows:
        path.write_text("group\n", encoding="utf-8")
        return

    fieldnames = list(rows[0].keys())
    with path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def _write_summary_markdown(
    *,
    summary: EvaluationSummary,
    group_metrics: Mapping[str, GroupMetrics],
    output_path: Path,
) -> None:
    lines: List[str] = [
        "# Fairness & Robustness Audit",
        "",
        f"Generated on {dt.datetime.utcnow().isoformat()} UTC",
        "",
        "## Overall evaluation",
        "",
        "| Metric | Value |",
        "| --- | --- |",
    ]

    for key in ["samples", "accuracy", "precision", "recall", "f1", "far", "frr"]:
        value = summary.metrics.get(key)
        if value is None:
            continue
        if isinstance(value, float):
            formatted = f"{value:.4f}"
        else:
            formatted = str(value)
        lines.append(f"| {key} | {formatted} |")

    for name, metrics in group_metrics.items():
        lines.extend(["", f"## {name.replace('_', ' ').title()}", ""])
        if not metrics.rows:
            lines.append("No samples available for this grouping.")
            continue

        headings = [
            "group",
            "samples",
            "accuracy",
            "precision",
            "recall",
            "f1",
            "far",
            "frr",
        ]
        lines.append("| " + " | ".join(headings) + " |")
        lines.append("| " + " | ".join(["---"] * len(headings)) + " |")
        for row in metrics.rows:
            formatted_row = []
            for heading in headings:
                value = row.get(heading, "")
                if isinstance(value, float):
                    formatted_row.append(f"{value:.4f}")
                else:
                    formatted_row.append(str(value))
            lines.append("| " + " | ".join(formatted_row) + " |")

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def run_fairness_audit(config: FairnessAuditConfig) -> FairnessAuditResult:
    """Execute the fairness audit and persist derived artifacts."""

    assert config.reports_dir is not None
    config.reports_dir.mkdir(parents=True, exist_ok=True)

    evaluation_config = config.to_evaluation_config()
    evaluation = run_face_recognition_evaluation(evaluation_config)

    if config.threshold is None:
        assert evaluation.metrics.get("threshold") is not None
        threshold = float(evaluation.metrics["threshold"])
    else:
        threshold = float(config.threshold)

    annotated_samples = annotate_samples(evaluation.samples)

    grouping_definitions: List[tuple[str, Callable[[SampleContext], str]]] = [
        ("metrics_by_role", lambda ctx: ctx.role_bucket),
        ("metrics_by_site", lambda ctx: ctx.site_bucket or "unspecified"),
        ("metrics_by_source", lambda ctx: ctx.source_bucket or "unspecified"),
        ("metrics_by_lighting", lambda ctx: ctx.lighting_bucket or "unknown"),
    ]

    group_metrics: Dict[str, GroupMetrics] = {}
    for name, getter in grouping_definitions:
        rows = compute_group_metrics(
            annotated_samples,
            threshold=threshold,
            group_name=name,
            value_getter=getter,
        )
        csv_path = config.reports_dir / f"{name}.csv"
        _write_group_csv(rows, csv_path)
        group_metrics[name] = GroupMetrics(name=name, rows=rows, csv_path=csv_path)

    summary_path = config.reports_dir / "summary.md"
    _write_summary_markdown(
        summary=evaluation, group_metrics=group_metrics, output_path=summary_path
    )

    return FairnessAuditResult(
        evaluation=evaluation,
        group_metrics=group_metrics,
        summary_path=summary_path,
    )


__all__ = [
    "AnnotatedSample",
    "FairnessAuditConfig",
    "FairnessAuditResult",
    "GroupMetrics",
    "annotate_samples",
    "compute_group_metrics",
    "estimate_lighting_bucket",
    "run_fairness_audit",
]
