"""Fairness and robustness metrics for the face-recognition pipeline."""

from __future__ import annotations

import csv
import datetime as dt
import logging
from collections import Counter, defaultdict
from dataclasses import dataclass, replace
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Mapping, Optional, Sequence

from django.conf import settings
from django.db.models import Count

from src.evaluation.face_recognition_eval import (
    UNKNOWN_LABEL,
    EvaluationConfig,
    EvaluationSummary,
    SampleEvaluation,
    compute_basic_metrics,
    run_face_recognition_evaluation,
)

logger = logging.getLogger(__name__)


@dataclass
class SampleContext:
    """Metadata describing a sample's audit group membership."""

    username: str
    role_bucket: str
    site_bucket: str
    source_bucket: str
    lighting_bucket: str


@dataclass
class AnnotatedSample:
    """Pair a :class:`SampleEvaluation` instance with its :class:`SampleContext`."""

    sample: SampleEvaluation
    context: SampleContext


@dataclass
class FairnessAuditConfig:
    """Configuration parameters for the fairness audit."""

    reports_dir: Optional[Path] = None
    dataset_root: Optional[Path] = None
    test_split_csv: Optional[Path] = None
    threshold: Optional[float] = None
    limit_samples: Optional[int] = None

    def __post_init__(self) -> None:
        if getattr(settings, "configured", False):
            base_dir = Path(getattr(settings, "BASE_DIR", Path.cwd()))
        else:
            base_dir = Path.cwd()
        if self.reports_dir is None:
            self.reports_dir = base_dir / "reports" / "fairness"
        else:
            self.reports_dir = Path(self.reports_dir)

        if self.dataset_root is not None:
            self.dataset_root = Path(self.dataset_root)
        if self.test_split_csv is not None:
            self.test_split_csv = Path(self.test_split_csv)
        if self.limit_samples is not None and self.limit_samples <= 0:
            raise ValueError("limit_samples must be positive when provided")

    @property
    def evaluation_reports_dir(self) -> Path:
        """Directory used when persisting intermediate evaluation outputs."""

        assert self.reports_dir is not None
        return self.reports_dir / "evaluation_snapshot"

    def to_evaluation_config(self) -> EvaluationConfig:
        """Translate the audit configuration into :class:`EvaluationConfig`."""

        return EvaluationConfig(
            reports_dir=self.evaluation_reports_dir,
            dataset_root=self.dataset_root,
            test_split_csv=self.test_split_csv,
            threshold=self.threshold,
            limit_samples=self.limit_samples,
        )


@dataclass
class GroupMetrics:
    """Container describing the metrics computed for a single grouping axis."""

    name: str
    rows: List[Dict[str, object]]
    csv_path: Path


@dataclass
class ThresholdRecommendation:
    """Recommended threshold adjustment for a specific group.

    Attributes:
        group_type: Type of group (lighting, role, site, source).
        group_value: Specific group identifier.
        current_threshold: The threshold used during evaluation.
        recommended_threshold: Suggested threshold for this group.
        adjustment_reason: Why this adjustment is recommended.
        far: Current False Accept Rate for this group.
        frr: Current False Reject Rate for this group.
        sample_count: Number of samples in this group.
    """

    group_type: str
    group_value: str
    current_threshold: float
    recommended_threshold: float
    adjustment_reason: str
    far: float
    frr: float
    sample_count: int


@dataclass
class FairnessAuditResult:
    """Outputs generated by :func:`run_fairness_audit`."""

    evaluation: EvaluationSummary
    group_metrics: Dict[str, GroupMetrics]
    summary_path: Path
    threshold_recommendations: List[ThresholdRecommendation] = None

    def __post_init__(self):
        if self.threshold_recommendations is None:
            self.threshold_recommendations = []


def estimate_lighting_bucket(image_path: Path) -> str:
    """Return a coarse lighting bucket for the provided image path."""

    try:
        import numpy as np
        from PIL import Image  # Imported lazily to avoid mandatory dependency during tests
    except Exception:  # pragma: no cover - Pillow is part of runtime requirements
        return "unknown"

    try:
        with Image.open(image_path) as img:
            grayscale = img.convert("L")
            data = np.asarray(grayscale, dtype=float)
    except Exception:
        return "unknown"

    if data.size == 0:
        return "unknown"

    mean_brightness = float(data.mean())
    if mean_brightness < 60:
        return "low_light"
    if mean_brightness < 160:
        return "moderate_light"
    return "bright_light"


def _bucketize_role(user) -> str:
    if user is None:
        return "unregistered"
    if user.is_superuser or user.is_staff:
        return "staff_or_admin"
    return "employee"


def _resolve_user_contexts(usernames: Iterable[str]) -> Dict[str, SampleContext]:
    """Return cached metadata describing each username's grouping context."""

    unique_usernames = sorted({name for name in usernames if name and name != UNKNOWN_LABEL})
    from django.contrib.auth import get_user_model

    from users.models import RecognitionAttempt

    user_model = get_user_model()
    user_map: Dict[str, SampleContext] = {}

    if not unique_usernames:
        return user_map

    users = user_model.objects.filter(username__in=unique_usernames)
    for user in users:
        user_map[user.username] = SampleContext(
            username=user.username,
            role_bucket=_bucketize_role(user),
            site_bucket="unspecified",
            source_bucket="unspecified",
            lighting_bucket="unknown",
        )

    # Ensure all usernames have at least a placeholder entry.
    for username in unique_usernames:
        user_map.setdefault(
            username,
            SampleContext(
                username=username,
                role_bucket="unregistered",
                site_bucket="unspecified",
                source_bucket="unspecified",
                lighting_bucket="unknown",
            ),
        )

    attempts = (
        RecognitionAttempt.objects.filter(username__in=unique_usernames)
        .exclude(site="")
        .values("username", "site")
        .annotate(total=Count("id"))
    )
    site_counts: Dict[str, Counter[str]] = defaultdict(Counter)
    for entry in attempts:
        site_counts[entry["username"]][entry["site"]] += entry["total"]

    source_attempts = (
        RecognitionAttempt.objects.filter(username__in=unique_usernames)
        .exclude(source="")
        .values("username", "source")
        .annotate(total=Count("id"))
    )
    source_counts: Dict[str, Counter[str]] = defaultdict(Counter)
    for entry in source_attempts:
        source_counts[entry["username"]][entry["source"]] += entry["total"]

    for username, context in user_map.items():
        if site_counts.get(username):
            context.site_bucket = site_counts[username].most_common(1)[0][0]
        if source_counts.get(username):
            context.source_bucket = source_counts[username].most_common(1)[0][0]

    return user_map


def annotate_samples(samples: Sequence[SampleEvaluation]) -> List[AnnotatedSample]:
    """Attach metadata-derived contexts to each evaluation sample."""

    usernames = [sample.ground_truth for sample in samples]
    context_map = _resolve_user_contexts(usernames)

    annotated: List[AnnotatedSample] = []
    default_contexts: Dict[str, SampleContext] = {}

    for sample in samples:
        username = sample.ground_truth
        if username not in context_map:
            default_contexts.setdefault(
                username,
                SampleContext(
                    username=username,
                    role_bucket="unregistered" if username != UNKNOWN_LABEL else UNKNOWN_LABEL,
                    site_bucket="unspecified",
                    source_bucket="unspecified",
                    lighting_bucket="unknown",
                ),
            )
        base_context = context_map.get(username, default_contexts[username])
        context = replace(base_context)
        context.lighting_bucket = estimate_lighting_bucket(sample.image_path)
        annotated.append(AnnotatedSample(sample=sample, context=context))

    return annotated


def _group_samples(
    annotated_samples: Sequence[AnnotatedSample],
    *,
    value_getter: Callable[[SampleContext], str],
) -> Dict[str, List[SampleEvaluation]]:
    grouped: Dict[str, List[SampleEvaluation]] = defaultdict(list)
    for annotated in annotated_samples:
        group_value = value_getter(annotated.context) or "unspecified"
        grouped[group_value].append(annotated.sample)
    return grouped


def compute_group_metrics(
    annotated_samples: Sequence[AnnotatedSample],
    *,
    threshold: float,
    group_name: str,
    value_getter: Callable[[SampleContext], str],
) -> List[Dict[str, object]]:
    """Compute metrics for each bucket defined by ``value_getter``."""

    grouped = _group_samples(annotated_samples, value_getter=value_getter)
    rows: List[Dict[str, object]] = []
    for group_value, group_samples in sorted(grouped.items()):
        if not group_samples:
            continue
        metrics, _, _, _ = compute_basic_metrics(group_samples, threshold)
        row: Dict[str, object] = {"group": group_value, **metrics}
        rows.append(row)

    logger.info("Computed %s buckets for %s", len(rows), group_name)
    return rows


def _write_group_csv(rows: Sequence[Mapping[str, object]], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if not rows:
        path.write_text("group\n", encoding="utf-8")
        return

    fieldnames = list(rows[0].keys())
    with path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def _write_summary_markdown(
    *,
    summary: EvaluationSummary,
    group_metrics: Mapping[str, GroupMetrics],
    output_path: Path,
) -> None:
    lines: List[str] = [
        "# Fairness & Robustness Audit",
        "",
        f"Generated on {dt.datetime.utcnow().isoformat()} UTC",
        "",
        "## Overall evaluation",
        "",
        "| Metric | Value |",
        "| --- | --- |",
    ]

    for key in ["samples", "accuracy", "precision", "recall", "f1", "far", "frr"]:
        value = summary.metrics.get(key)
        if value is None:
            continue
        if isinstance(value, float):
            formatted = f"{value:.4f}"
        else:
            formatted = str(value)
        lines.append(f"| {key} | {formatted} |")

    for name, metrics in group_metrics.items():
        lines.extend(["", f"## {name.replace('_', ' ').title()}", ""])
        if not metrics.rows:
            lines.append("No samples available for this grouping.")
            continue

        headings = [
            "group",
            "samples",
            "accuracy",
            "precision",
            "recall",
            "f1",
            "far",
            "frr",
        ]
        lines.append("| " + " | ".join(headings) + " |")
        lines.append("| " + " | ".join(["---"] * len(headings)) + " |")
        for row in metrics.rows:
            formatted_row = []
            for heading in headings:
                value = row.get(heading, "")
                if isinstance(value, float):
                    formatted_row.append(f"{value:.4f}")
                else:
                    formatted_row.append(str(value))
            lines.append("| " + " | ".join(formatted_row) + " |")

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def run_fairness_audit(config: FairnessAuditConfig) -> FairnessAuditResult:
    """Execute the fairness audit and persist derived artifacts."""

    assert config.reports_dir is not None
    config.reports_dir.mkdir(parents=True, exist_ok=True)

    evaluation_config = config.to_evaluation_config()
    evaluation = run_face_recognition_evaluation(evaluation_config)

    if config.threshold is None:
        assert evaluation.metrics.get("threshold") is not None
        threshold = float(evaluation.metrics["threshold"])
    else:
        threshold = float(config.threshold)

    annotated_samples = annotate_samples(evaluation.samples)

    grouping_definitions: List[tuple[str, Callable[[SampleContext], str]]] = [
        ("metrics_by_role", lambda ctx: ctx.role_bucket),
        ("metrics_by_site", lambda ctx: ctx.site_bucket or "unspecified"),
        ("metrics_by_source", lambda ctx: ctx.source_bucket or "unspecified"),
        ("metrics_by_lighting", lambda ctx: ctx.lighting_bucket or "unknown"),
    ]

    group_metrics: Dict[str, GroupMetrics] = {}
    for name, getter in grouping_definitions:
        rows = compute_group_metrics(
            annotated_samples,
            threshold=threshold,
            group_name=name,
            value_getter=getter,
        )
        csv_path = config.reports_dir / f"{name}.csv"
        _write_group_csv(rows, csv_path)
        group_metrics[name] = GroupMetrics(name=name, rows=rows, csv_path=csv_path)

    summary_path = config.reports_dir / "summary.md"
    _write_summary_markdown(
        summary=evaluation, group_metrics=group_metrics, output_path=summary_path
    )

    return FairnessAuditResult(
        evaluation=evaluation,
        group_metrics=group_metrics,
        summary_path=summary_path,
    )


def compute_threshold_recommendations(
    group_metrics: Dict[str, GroupMetrics],
    current_threshold: float,
    *,
    frr_threshold: float = 0.15,
    far_threshold: float = 0.05,
    min_samples: int = 10,
) -> List[ThresholdRecommendation]:
    """Generate threshold recommendations for groups with elevated error rates.

    Args:
        group_metrics: Metrics computed per group by the fairness audit.
        current_threshold: The threshold used during evaluation.
        frr_threshold: FRR above this triggers a looser threshold recommendation.
        far_threshold: FAR above this triggers a stricter threshold recommendation.
        min_samples: Minimum samples required to make a recommendation.

    Returns:
        List of threshold recommendations for groups needing adjustment.
    """
    recommendations: List[ThresholdRecommendation] = []

    group_type_mapping = {
        "metrics_by_role": "role",
        "metrics_by_site": "site",
        "metrics_by_source": "camera",
        "metrics_by_lighting": "lighting",
    }

    for metric_name, metrics in group_metrics.items():
        group_type = group_type_mapping.get(metric_name, "")
        if not group_type:
            continue

        for row in metrics.rows:
            group_value = str(row.get("group", ""))
            sample_count = int(row.get("samples", 0))
            far = float(row.get("far", 0.0))
            frr = float(row.get("frr", 0.0))

            if sample_count < min_samples:
                continue  # Not enough data for reliable recommendation

            adjustment_reason = ""
            recommended = current_threshold

            if frr > frr_threshold:
                # High False Reject Rate - recommend looser threshold
                adjustment = min(0.1, frr * 0.3)  # Scale adjustment to FRR
                recommended = current_threshold + adjustment
                adjustment_reason = (
                    f"High FRR ({frr:.1%}) - loosening threshold to reduce rejections"
                )
            elif far > far_threshold:
                # High False Accept Rate - recommend stricter threshold
                adjustment = min(0.1, far * 0.5)  # Scale adjustment to FAR
                recommended = max(0.1, current_threshold - adjustment)
                adjustment_reason = (
                    f"High FAR ({far:.1%}) - tightening threshold for security"
                )

            if adjustment_reason:
                recommendations.append(
                    ThresholdRecommendation(
                        group_type=group_type,
                        group_value=group_value,
                        current_threshold=current_threshold,
                        recommended_threshold=round(recommended, 4),
                        adjustment_reason=adjustment_reason,
                        far=far,
                        frr=frr,
                        sample_count=sample_count,
                    )
                )

    return recommendations


def write_threshold_recommendations_csv(
    recommendations: List[ThresholdRecommendation],
    output_path: Path,
) -> None:
    """Write threshold recommendations to a CSV file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fieldnames = [
        "group_type",
        "group_value",
        "current_threshold",
        "recommended_threshold",
        "adjustment_reason",
        "far",
        "frr",
        "sample_count",
    ]

    with output_path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for rec in recommendations:
            writer.writerow({
                "group_type": rec.group_type,
                "group_value": rec.group_value,
                "current_threshold": rec.current_threshold,
                "recommended_threshold": rec.recommended_threshold,
                "adjustment_reason": rec.adjustment_reason,
                "far": f"{rec.far:.4f}",
                "frr": f"{rec.frr:.4f}",
                "sample_count": rec.sample_count,
            })


__all__ = [
    "AnnotatedSample",
    "FairnessAuditConfig",
    "FairnessAuditResult",
    "GroupMetrics",
    "ThresholdRecommendation",
    "annotate_samples",
    "compute_group_metrics",
    "compute_threshold_recommendations",
    "estimate_lighting_bucket",
    "run_fairness_audit",
    "write_threshold_recommendations_csv",
]
