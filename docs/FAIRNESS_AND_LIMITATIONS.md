# Fairness, Robustness, and Known Limitations

This document captures the current status of the face-recognition fairness and robustness audit. The goal is to provide a reproducible measurement of how the matcher behaves for different operational groups (role, site, source) and environmental conditions (coarse lighting buckets) without storing or inferring sensitive attributes. The findings here complement the [DATA_CARD.md](../DATA_CARD.md), which documents how the dataset is collected and governed.

## Methodology

1. **Evaluation set** – The audit reuses the encrypted `face_recognition_data/training_dataset/` tree and, when available, the test split described in `reports/splits.csv` generated by `manage.py prepare_splits`. No production attendance entries are modified during the run.
2. **Recognition pipeline** – We reuse the exact DeepFace configuration (model, detector, distance metric, and decision threshold) configured for production to guarantee that fairness measurements match real-world behaviour.
3. **Grouping axes**
   - **Role proxy** – Whether a Django user is flagged as `is_staff`/`is_superuser` (bucketed as `staff_or_admin`) or a regular employee (`employee`). Unknown usernames fall into the `unregistered` bucket.
   - **Operational context** – For each username we look at historical `users.RecognitionAttempt` records to infer their most common `site` and `source` strings. This highlights site-specific camera issues or modality drift between the browser webcam, kiosk devices, or API clients.
   - **Lighting heuristic** – Each evaluation image is converted to grayscale and bucketed into `low_light`, `moderate_light`, or `bright_light` based on mean pixel intensity.
4. **Metrics** – For each bucket we compute accuracy, precision, recall, macro F1, False Acceptance Rate (FAR), False Rejection Rate (FRR), and the total number of processed samples.

## Running the audit

```bash
python manage.py fairness_audit \
  --split-csv reports/splits.csv \
  --reports-dir reports/fairness
```

Options:
- `--dataset-root` points to an alternate dataset tree when you need to run the audit outside of the default encrypted directory.
- `--threshold` lets you test sensitivity to different distance cutoffs.
- `--max-samples` limits the run to a quick smoke test when hardware resources are constrained.

The command writes the following artifacts (relative to `reports/fairness/`):

| File | Purpose |
| --- | --- |
| `summary.md` | Human-readable overview with per-group tables and a snapshot of the aggregate metrics from the underlying evaluation run. |
| `metrics_by_role.csv` | Machine-readable metrics for `staff_or_admin`, `employee`, and `unregistered` buckets. |
| `metrics_by_site.csv` | Accuracy/FAR/FRR for each observed site label inferred from `users.RecognitionAttempt`. |
| `metrics_by_source.csv` | Similar metrics for the capture source (browser webcam, API, kiosk, etc.). |
| `metrics_by_lighting.csv` | Robustness slice highlighting performance deltas between low-, medium-, and bright-light captures. |
| `evaluation_snapshot/` | Full outputs from `manage.py eval`, stored so auditors can inspect raw predictions without rerunning the expensive inference step. |

## Findings

- A successful audit produces per-bucket summaries in `reports/fairness/summary.md`. These tables list sample counts and metric deltas so incident responders can spot regressions (for example, elevated FRR for `low_light`).
- Since the grouping axes are proxies (role, dominant site/source, heuristic lighting), the report is best interpreted as an operational health check rather than a demographic fairness guarantee. Whenever new metadata becomes available it should be wired into `src/evaluation/fairness.py` so the audit can capture the additional slices.

## Limitations and next steps

1. **Demographic coverage** – The system intentionally does not persist sensitive attributes (gender, ethnicity, age). As a result, we cannot quantify demographic fairness directly. External red-teaming or synthetic benchmarks are recommended before deploying to populations with substantially different characteristics than the current staff roll.
2. **Small buckets** – Some sites and sources may only have a handful of samples. The CSV exports include the `samples` column so reviewers can avoid over-interpreting metrics derived from very low support. Consider aggregating sites with fewer than 10 samples before drawing conclusions.
3. **Lighting heuristic** – The grayscale mean is a coarse proxy. It surfaces obvious dark vs. bright failures but it cannot distinguish shadows, glare, or occlusions such as masks and glasses. Extending the dataset with labelled conditions would enable a richer robustness audit.
4. **Temporal drift** – The audit runs on static evaluation snapshots. If cameras move or new sensors are introduced, rerun `manage.py fairness_audit` after generating a new test split so the report reflects the latest conditions.

For any deployment that extends the dataset or introduces new sensors, update this document and [DATA_CARD.md](../DATA_CARD.md) with the new context so downstream users understand the coverage and blind spots.
