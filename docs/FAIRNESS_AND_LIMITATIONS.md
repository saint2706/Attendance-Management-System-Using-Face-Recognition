# Fairness, Robustness, and Known Limitations

This document captures the current status of the face-recognition fairness and robustness audit. The goal is to provide a reproducible measurement of how the matcher behaves for different operational groups (role, site, source) and environmental conditions (coarse lighting buckets) without storing or inferring sensitive attributes. The findings here complement the [DATA_CARD.md](../DATA_CARD.md), which documents how the dataset is collected and governed.

## Methodology

1. **Evaluation set** – The audit reuses the encrypted `face_recognition_data/training_dataset/` tree and, when available, the test split described in `reports/splits.csv` generated by `manage.py prepare_splits`. No production attendance entries are modified during the run.
2. **Recognition pipeline** – We reuse the exact DeepFace configuration (model, detector, distance metric, and decision threshold) configured for production to guarantee that fairness measurements match real-world behaviour.
3. **Grouping axes**
   - **Role proxy** – Whether a Django user is flagged as `is_staff`/`is_superuser` (bucketed as `staff_or_admin`) or a regular employee (`employee`). Unknown usernames fall into the `unregistered` bucket.
   - **Operational context** – For each username we look at historical `users.RecognitionAttempt` records to infer their most common `site` and `source` strings. This highlights site-specific camera issues or modality drift between the browser webcam, kiosk devices, or API clients.
   - **Lighting heuristic** – Each evaluation image is converted to grayscale and bucketed into `low_light`, `moderate_light`, or `bright_light` based on mean pixel intensity.
4. **Metrics** – For each bucket we compute accuracy, precision, recall, macro F1, False Acceptance Rate (FAR), False Rejection Rate (FRR), and the total number of processed samples.

## Running the audit

```bash
python manage.py fairness_audit \
  --split-csv reports/splits.csv \
  --reports-dir reports/fairness
```

Options:

- `--dataset-root` points to an alternate dataset tree when you need to run the audit outside of the default encrypted directory.
- `--threshold` lets you test sensitivity to different distance cutoffs.
- `--max-samples` limits the run to a quick smoke test when hardware resources are constrained.
- `--recommend-thresholds` generates per-group threshold recommendations based on FAR/FRR metrics.
- `--frr-threshold` sets the FRR threshold above which a looser threshold is recommended (default: 0.15).
- `--far-threshold` sets the FAR threshold above which a stricter threshold is recommended (default: 0.05).

The command writes the following artifacts (relative to `reports/fairness/`):

| File | Purpose |
| --- | --- |
| `summary.md` | Human-readable overview with per-group tables and a snapshot of the aggregate metrics from the underlying evaluation run. |
| `metrics_by_role.csv` | Machine-readable metrics for `staff_or_admin`, `employee`, and `unregistered` buckets. |
| `metrics_by_site.csv` | Accuracy/FAR/FRR for each observed site label inferred from `users.RecognitionAttempt`. |
| `metrics_by_source.csv` | Similar metrics for the capture source (browser webcam, API, kiosk, etc.). |
| `metrics_by_lighting.csv` | Robustness slice highlighting performance deltas between low-, medium-, and bright-light captures. |
| `threshold_recommendations.csv` | Per-group threshold adjustment recommendations (when `--recommend-thresholds` is used). |
| `evaluation_snapshot/` | Full outputs from `manage.py eval`, stored so auditors can inspect raw predictions without rerunning the expensive inference step. |

## Threshold Recommendations

When groups show elevated error rates, the `--recommend-thresholds` flag generates actionable recommendations:

- **High FRR groups** (>15% False Reject Rate) receive a recommendation to loosen the threshold, reducing frustration for legitimate users in challenging conditions (e.g., low lighting).
- **High FAR groups** (>5% False Accept Rate) receive a recommendation to tighten the threshold, improving security.

Recommendations can be applied using the `ThresholdProfile` model which supports per-group thresholds:

```bash
# Create a threshold profile for low-light conditions
python manage.py threshold_profile create \
  --name "low_light_adjusted" \
  --threshold 0.45 \
  --group-type lighting \
  --group-value low_light
```

## Domain Adaptation for Multiple Cameras

When deploying across different cameras (webcams, kiosks, mobile devices), use the domain adaptation tools to assess and mitigate camera-specific biases:

```bash
# Calibrate a new camera
python manage.py calibrate_camera lobby_kiosk \
  --images-dir /path/to/calibration/images \
  --compare-to reception_webcam
```

The calibration process:

1. Analyzes reference images to estimate camera characteristics (brightness, contrast, color temperature)
2. Compares the new camera to existing profiles to identify domain gaps
3. Provides recommendations for threshold adjustments or image normalization

See [TRAINING_PROTOCOL.md](TRAINING_PROTOCOL.md) for guidance on collecting diverse training data across varied lighting and poses.

## Findings

- A successful audit produces per-bucket summaries in `reports/fairness/summary.md`. These tables list sample counts and metric deltas so incident responders can spot regressions (for example, elevated FRR for `low_light`).
- Since the grouping axes are proxies (role, dominant site/source, heuristic lighting), the report is best interpreted as an operational health check rather than a demographic fairness guarantee. Whenever new metadata becomes available it should be wired into `src/evaluation/fairness.py` so the audit can capture the additional slices.

## Limitations and next steps

1. **Demographic coverage** – The system intentionally does not persist sensitive attributes (gender, ethnicity, age). As a result, we cannot quantify demographic fairness directly. External red-teaming or synthetic benchmarks are recommended before deploying to populations with substantially different characteristics than the current staff roll.
2. **Small buckets** – Some sites and sources may only have a handful of samples. The CSV exports include the `samples` column so reviewers can avoid over-interpreting metrics derived from very low support. Consider aggregating sites with fewer than 10 samples before drawing conclusions.
3. **Lighting heuristic** – The grayscale mean is a coarse proxy. It surfaces obvious dark vs. bright failures but it cannot distinguish shadows, glare, or occlusions such as masks and glasses. Extending the dataset with labelled conditions would enable a richer robustness audit.
4. **Temporal drift** – The audit runs on static evaluation snapshots. If cameras move or new sensors are introduced, rerun `manage.py fairness_audit` after generating a new test split so the report reflects the latest conditions.

For any deployment that extends the dataset or introduces new sensors, update this document and [DATA_CARD.md](../DATA_CARD.md) with the new context so downstream users understand the coverage and blind spots.
