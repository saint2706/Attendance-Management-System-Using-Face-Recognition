# Generated by Django 5.1.14 on 2025-11-29 11:55

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("recognition", "0002_add_threshold_profiles_and_liveness"),
    ]

    operations = [
        migrations.CreateModel(
            name="ModelEvaluationResult",
            fields=[
                (
                    "id",
                    models.BigAutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("created_at", models.DateTimeField(auto_now_add=True, db_index=True)),
                (
                    "evaluation_type",
                    models.CharField(
                        choices=[
                            ("nightly", "Scheduled Nightly"),
                            ("weekly", "Scheduled Weekly"),
                            ("fairness", "Fairness Audit"),
                            ("liveness", "Liveness Evaluation"),
                            ("manual", "Manual Evaluation"),
                        ],
                        default="manual",
                        max_length=16,
                    ),
                ),
                (
                    "accuracy",
                    models.FloatField(
                        blank=True, help_text="Overall accuracy of the model", null=True
                    ),
                ),
                (
                    "precision",
                    models.FloatField(blank=True, help_text="Weighted precision score", null=True),
                ),
                (
                    "recall",
                    models.FloatField(blank=True, help_text="Weighted recall score", null=True),
                ),
                (
                    "f1_score",
                    models.FloatField(blank=True, help_text="Weighted F1 score", null=True),
                ),
                ("far", models.FloatField(blank=True, help_text="False Accept Rate", null=True)),
                ("frr", models.FloatField(blank=True, help_text="False Reject Rate", null=True)),
                (
                    "samples_evaluated",
                    models.PositiveIntegerField(
                        default=0, help_text="Number of samples used for evaluation"
                    ),
                ),
                (
                    "threshold_used",
                    models.FloatField(
                        blank=True, help_text="Distance threshold used for evaluation", null=True
                    ),
                ),
                (
                    "identities_evaluated",
                    models.PositiveIntegerField(
                        default=0, help_text="Number of unique identities in evaluation set"
                    ),
                ),
                (
                    "liveness_pass_rate",
                    models.FloatField(
                        blank=True, help_text="Liveness check pass rate (0.0 to 1.0)", null=True
                    ),
                ),
                (
                    "liveness_samples",
                    models.PositiveIntegerField(
                        default=0, help_text="Number of liveness checks evaluated"
                    ),
                ),
                (
                    "task_id",
                    models.CharField(
                        blank=True,
                        help_text="Celery task ID that generated this result",
                        max_length=255,
                    ),
                ),
                (
                    "duration_seconds",
                    models.FloatField(
                        blank=True,
                        help_text="Time taken to run the evaluation in seconds",
                        null=True,
                    ),
                ),
                (
                    "error_message",
                    models.TextField(blank=True, help_text="Error message if evaluation failed"),
                ),
                (
                    "success",
                    models.BooleanField(
                        default=True, help_text="Whether the evaluation completed successfully"
                    ),
                ),
            ],
            options={
                "verbose_name": "Model Evaluation Result",
                "verbose_name_plural": "Model Evaluation Results",
                "ordering": ["-created_at"],
                "indexes": [
                    models.Index(
                        fields=["created_at", "evaluation_type"],
                        name="recognition_created_cffd0c_idx",
                    ),
                    models.Index(
                        fields=["evaluation_type", "success"], name="recognition_evaluat_0182d5_idx"
                    ),
                ],
            },
        ),
    ]
